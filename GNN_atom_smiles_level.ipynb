{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yufanlili211/master_thesis/blob/main/GNN_atom_smiles_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLK_PY5MMxy2"
      },
      "source": [
        "peptide sequence to smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01s2oOKEMscV",
        "outputId": "8995c6fb-e4d2-47e5-88a9-23756816233b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.5-cp312-cp312-manylinux_2_28_x86_64.whl (36.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.5\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_scatter-2.1.2%2Bpt23cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt23cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_sparse-0.6.18%2Bpt23cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt23cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install rdkit # install rdkit\n",
        "!pip install --no-cache-dir torch-scatter -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "!pip install --no-cache-dir torch-sparse -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "!pip install --no-cache-dir torch-geometric -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load residue dictionary\n",
        "# -----------------------------\n",
        "# Expected columns in Excel:\n",
        "# - one column for amino-acid code (e.g., A, C, D)\n",
        "# - one column for capped residue smiles (Ac-Res-NMe)\n",
        "\n",
        "def load_residue_dictionary_from_excel(\n",
        "    excel_path,\n",
        "    code_col='ID',\n",
        "    smiles_col='SMILES (Ac-Res-NMe)',\n",
        "):\n",
        "    df = pd.read_excel(excel_path)\n",
        "    if code_col not in df.columns or smiles_col not in df.columns:\n",
        "        raise ValueError(\n",
        "            f\"Missing required columns. Found: {list(df.columns)}; \"\n",
        "            f\"need code_col='{code_col}', smiles_col='{smiles_col}'.\"\n",
        "        )\n",
        "    mapping = {\n",
        "        str(row[code_col]).strip(): str(row[smiles_col]).strip()\n",
        "        for _, row in df.iterrows()\n",
        "        if pd.notna(row[code_col]) and pd.notna(row[smiles_col])\n",
        "    }\n",
        "    return mapping\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Feature vocabularies\n",
        "# -----------------------------\n",
        "# All categorical values are converted to integer ids for nn.Embedding.\n",
        "\n",
        "ATOM_TYPE_VOCAB = [\n",
        "    'H', 'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'Br', 'I', 'Se', 'UNK'\n",
        "]\n",
        "CHIRALITY_VOCAB = [\n",
        "    str(Chem.rdchem.ChiralType.CHI_UNSPECIFIED),\n",
        "    str(Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW),\n",
        "    str(Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW),\n",
        "    str(Chem.rdchem.ChiralType.CHI_OTHER),\n",
        "    'UNK',\n",
        "]\n",
        "DEGREE_VOCAB = [0, 1, 2, 3, 4, 5, 6, 'UNK']\n",
        "FORMAL_CHARGE_VOCAB = [-3, -2, -1, 0, 1, 2, 3, 'UNK']\n",
        "NUM_H_VOCAB = [0, 1, 2, 3, 4, 'UNK']\n",
        "RADICAL_E_VOCAB = [0, 1, 2, 'UNK']\n",
        "HYBRIDIZATION_VOCAB = [\n",
        "    str(Chem.rdchem.HybridizationType.SP),\n",
        "    str(Chem.rdchem.HybridizationType.SP2),\n",
        "    str(Chem.rdchem.HybridizationType.SP3),\n",
        "    str(Chem.rdchem.HybridizationType.SP3D),\n",
        "    str(Chem.rdchem.HybridizationType.SP3D2),\n",
        "    'UNK',\n",
        "]\n",
        "BOOL_VOCAB = [False, True]\n",
        "\n",
        "BOND_TYPE_VOCAB = [\n",
        "    str(Chem.rdchem.BondType.SINGLE),\n",
        "    str(Chem.rdchem.BondType.DOUBLE),\n",
        "    str(Chem.rdchem.BondType.TRIPLE),\n",
        "    str(Chem.rdchem.BondType.AROMATIC),\n",
        "    'UNK',\n",
        "]\n",
        "BOND_STEREO_VOCAB = [\n",
        "    str(Chem.rdchem.BondStereo.STEREONONE),\n",
        "    str(Chem.rdchem.BondStereo.STEREOANY),\n",
        "    str(Chem.rdchem.BondStereo.STEREOZ),\n",
        "    str(Chem.rdchem.BondStereo.STEREOE),\n",
        "    str(Chem.rdchem.BondStereo.STEREOCIS),\n",
        "    str(Chem.rdchem.BondStereo.STEREOTRANS),\n",
        "    'UNK',\n",
        "]\n",
        "\n",
        "\n",
        "def _to_idx(value, vocab):\n",
        "    return vocab.index(value) if value in vocab else vocab.index('UNK')\n",
        "\n",
        "\n",
        "def atom_features(atom):\n",
        "    atom_symbol = atom.GetSymbol()\n",
        "    if atom_symbol not in ATOM_TYPE_VOCAB:\n",
        "        atom_symbol = 'UNK'\n",
        "\n",
        "    chirality = str(atom.GetChiralTag())\n",
        "    if chirality not in CHIRALITY_VOCAB:\n",
        "        chirality = 'UNK'\n",
        "\n",
        "    degree = atom.GetTotalDegree()\n",
        "    if degree not in DEGREE_VOCAB:\n",
        "        degree = 'UNK'\n",
        "\n",
        "    formal_charge = atom.GetFormalCharge()\n",
        "    if formal_charge not in FORMAL_CHARGE_VOCAB:\n",
        "        formal_charge = 'UNK'\n",
        "\n",
        "    num_h = atom.GetTotalNumHs(includeNeighbors=True)\n",
        "    if num_h not in NUM_H_VOCAB:\n",
        "        num_h = 'UNK'\n",
        "\n",
        "    radical_e = atom.GetNumRadicalElectrons()\n",
        "    if radical_e not in RADICAL_E_VOCAB:\n",
        "        radical_e = 'UNK'\n",
        "\n",
        "    hybrid = str(atom.GetHybridization())\n",
        "    if hybrid not in HYBRIDIZATION_VOCAB:\n",
        "        hybrid = 'UNK'\n",
        "\n",
        "    is_aromatic = atom.GetIsAromatic()\n",
        "    in_ring = atom.IsInRing()\n",
        "\n",
        "    return [\n",
        "        _to_idx(atom_symbol, ATOM_TYPE_VOCAB),\n",
        "        _to_idx(chirality, CHIRALITY_VOCAB),\n",
        "        DEGREE_VOCAB.index(degree),\n",
        "        FORMAL_CHARGE_VOCAB.index(formal_charge),\n",
        "        NUM_H_VOCAB.index(num_h),\n",
        "        RADICAL_E_VOCAB.index(radical_e),\n",
        "        HYBRIDIZATION_VOCAB.index(hybrid),\n",
        "        BOOL_VOCAB.index(is_aromatic),\n",
        "        BOOL_VOCAB.index(in_ring),\n",
        "    ]\n",
        "\n",
        "\n",
        "def bond_features(bond):\n",
        "    btype = str(bond.GetBondType())\n",
        "    if btype not in BOND_TYPE_VOCAB:\n",
        "        btype = 'UNK'\n",
        "\n",
        "    stereo = str(bond.GetStereo())\n",
        "    if stereo not in BOND_STEREO_VOCAB:\n",
        "        stereo = 'UNK'\n",
        "\n",
        "    return [\n",
        "        _to_idx(btype, BOND_TYPE_VOCAB),\n",
        "        _to_idx(stereo, BOND_STEREO_VOCAB),\n",
        "        BOOL_VOCAB.index(bond.GetIsConjugated()),\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPNUZWEuNAM3"
      },
      "source": [
        "smiles to atom-level graph features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HQqm9qfNIJT"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import rdchem\n",
        "\n",
        "\n",
        "def _find_backbone_atoms_and_caps(mol):\n",
        "    \"\"\"\n",
        "    Detect Ac-Res-NMe pattern and return key indices.\n",
        "\n",
        "    Pattern target: CH3-C(=O)-N-*-C(=O)-N-CH3\n",
        "    Keep: backbone N (left) and backbone carbonyl C (right)\n",
        "    Remove: Ac atoms (CH3-C=O) and NMe atoms (N-CH3)\n",
        "    \"\"\"\n",
        "    patt = Chem.MolFromSmarts('[CH3:1]-[C:2](=[O:3])-[N:4]-[*:5]-[C:6](=[O:7])-[N:8]-[CH3:9]')\n",
        "    matches = mol.GetSubstructMatches(patt)\n",
        "    if not matches:\n",
        "        raise ValueError('Cannot detect Ac-Res-NMe backbone pattern in residue SMILES.')\n",
        "\n",
        "    # Use first match; if you have ambiguous templates, refine SMARTS.\n",
        "    m = matches[0]\n",
        "    ac_methyl, ac_carb, ac_oxy = m[0], m[1], m[2]\n",
        "    backbone_n = m[3]\n",
        "    c_term_carb = m[5]\n",
        "    nme_n, nme_methyl = m[7], m[8]\n",
        "\n",
        "    remove_set = {ac_methyl, ac_carb, ac_oxy, nme_n, nme_methyl}\n",
        "    return backbone_n, c_term_carb, remove_set\n",
        "\n",
        "\n",
        "def _remove_atoms_and_map(mol, remove_set):\n",
        "    \"\"\"Remove a set of atom indices and return new molecule + old->new index mapping.\"\"\"\n",
        "    rw = Chem.RWMol(mol)\n",
        "    for idx in sorted(remove_set, reverse=True):\n",
        "        rw.RemoveAtom(idx)\n",
        "\n",
        "    old_to_new = {}\n",
        "    shift = 0\n",
        "    remove_sorted = sorted(remove_set)\n",
        "    j = 0\n",
        "    n = mol.GetNumAtoms()\n",
        "    for old_idx in range(n):\n",
        "        while j < len(remove_sorted) and remove_sorted[j] < old_idx:\n",
        "            j += 1\n",
        "        if old_idx in remove_set:\n",
        "            continue\n",
        "        removed_before = sum(1 for r in remove_sorted if r < old_idx)\n",
        "        old_to_new[old_idx] = old_idx - removed_before\n",
        "\n",
        "    mol2 = rw.GetMol()\n",
        "    Chem.SanitizeMol(mol2)\n",
        "    return mol2, old_to_new\n",
        "\n",
        "\n",
        "def _decap_residue(smiles):\n",
        "    \"\"\"\n",
        "    Convert Ac-Res-NMe to decapped residue fragment with open valences at backbone N and C(=O).\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        raise ValueError(f'Invalid residue SMILES: {smiles}')\n",
        "\n",
        "    n_idx, c_idx, remove_set = _find_backbone_atoms_and_caps(mol)\n",
        "    mol2, old_to_new = _remove_atoms_and_map(mol, remove_set)\n",
        "\n",
        "    if n_idx not in old_to_new or c_idx not in old_to_new:\n",
        "        raise ValueError('Backbone N or C was removed unexpectedly during decapping.')\n",
        "\n",
        "    n_new = old_to_new[n_idx]\n",
        "    c_new = old_to_new[c_idx]\n",
        "    return mol2, n_new, c_new\n",
        "\n",
        "\n",
        "def _combine_two_residues(mol_a, c_idx_a, mol_b, n_idx_b):\n",
        "    \"\"\"\n",
        "    Combine two residue fragments and form peptide bond C(=O)-N.\n",
        "    \"\"\"\n",
        "    combo = Chem.CombineMols(mol_a, mol_b)\n",
        "    rw = Chem.RWMol(combo)\n",
        "\n",
        "    offset = mol_a.GetNumAtoms()\n",
        "    n_idx_b_shifted = n_idx_b + offset\n",
        "\n",
        "    if rw.GetBondBetweenAtoms(c_idx_a, n_idx_b_shifted) is None:\n",
        "        rw.AddBond(c_idx_a, n_idx_b_shifted, rdchem.BondType.SINGLE)\n",
        "\n",
        "    new_mol = rw.GetMol()\n",
        "    Chem.SanitizeMol(new_mol)\n",
        "    return new_mol\n",
        "\n",
        "\n",
        "def molecule_to_pyg_data(mol):\n",
        "    \"\"\"Convert RDKit mol to torch_geometric Data with atom/bond categorical indices.\"\"\"\n",
        "    x = [atom_features(atom) for atom in mol.GetAtoms()]\n",
        "    x = torch.tensor(x, dtype=torch.long)\n",
        "\n",
        "    edge_indices = []\n",
        "    edge_attrs = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        bf = bond_features(bond)\n",
        "\n",
        "        edge_indices.append([i, j])\n",
        "        edge_indices.append([j, i])\n",
        "        edge_attrs.append(bf)\n",
        "        edge_attrs.append(bf)\n",
        "\n",
        "    if len(edge_indices) == 0:\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "        edge_attr = torch.empty((0, 3), dtype=torch.long)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attrs, dtype=torch.long)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "\n",
        "def build_peptide_graph(sequence, res_dict):\n",
        "    \"\"\"\n",
        "    Build an atom-level peptide graph from 1-letter sequence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequence : str\n",
        "        e.g., 'ACD'\n",
        "    res_dict : dict\n",
        "        maps AA code -> capped residue smiles (Ac-Res-NMe)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch_geometric.data.Data\n",
        "        with x, edge_index, edge_attr\n",
        "    \"\"\"\n",
        "    if not sequence:\n",
        "        raise ValueError('Sequence is empty.')\n",
        "\n",
        "    residue_items = []\n",
        "    for aa in sequence:\n",
        "        if aa not in res_dict:\n",
        "            raise KeyError(f\"Residue '{aa}' not found in dictionary.\")\n",
        "        mol_i, n_i, c_i = _decap_residue(res_dict[aa])\n",
        "        residue_items.append((mol_i, n_i, c_i))\n",
        "\n",
        "    current_mol, _, current_c = residue_items[0]\n",
        "\n",
        "    for i in range(1, len(residue_items)):\n",
        "        next_mol, next_n, next_c = residue_items[i]\n",
        "\n",
        "        prev_atoms = current_mol.GetNumAtoms()\n",
        "        current_mol = _combine_two_residues(current_mol, current_c, next_mol, next_n)\n",
        "        current_c = next_c + prev_atoms\n",
        "\n",
        "    Chem.SanitizeMol(current_mol)\n",
        "    return molecule_to_pyg_data(current_mol)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atom_dataset_build"
      },
      "source": [
        "build atom-level graph dataset (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22KGO04OU5Dn",
        "outputId": "c1c582ec-0b95-4afb-f8d7-98e095601eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atom_dataset_build_code",
        "outputId": "ae0e83e4-42d6-4c11-e033-b190b9293cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 5000 rows from peptide excel\n",
            "Using sequence column: aa_seq\n",
            "Using label column: seed_bh\n",
            "Built 5000 graphs (dropped=0)\n",
            "Saved splits to: /content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles\n",
            "train/val/test = 3500/750/750\n",
            "Sample x shape: torch.Size([154, 9])\n",
            "Sample edge_attr shape: torch.Size([312, 3])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "RESIDUAL_DICT_XLSX = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/residual_dictionary.xlsx\"\n",
        "PEPTIDE_EXCEL_PATH = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/canya_data_sampled_5000_smiles.xlsx\"\n",
        "SAVE_DIR = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles\"\n",
        "\n",
        "# Update only if your AA code column is different.\n",
        "AA_CODE_COL = \"ID\"\n",
        "AA_SMILES_COL = \"SMILES (Ac-Res-NMe)\"\n",
        "\n",
        "# You can set these explicitly if needed; otherwise auto-detect is used.\n",
        "SEQUENCE_COL = \"aa_seq\"\n",
        "LABEL_COL = \"seed_bh\"\n",
        "\n",
        "\n",
        "def _pick_column(df, explicit_name, candidates, kind):\n",
        "    if explicit_name is not None:\n",
        "        if explicit_name not in df.columns:\n",
        "            raise ValueError(f\"{kind} column '{explicit_name}' not found. Available: {list(df.columns)}\")\n",
        "        return explicit_name\n",
        "\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "\n",
        "    raise ValueError(\n",
        "        f\"Cannot auto-detect {kind} column. Available columns: {list(df.columns)}. \"\n",
        "        f\"Please set {kind.upper()}_COL explicitly.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def load_peptide_rows(peptide_excel_path, sequence_col=None, label_col=None):\n",
        "    df = pd.read_excel(peptide_excel_path)\n",
        "\n",
        "    seq_col = _pick_column(\n",
        "        df,\n",
        "        sequence_col,\n",
        "        [\"sequence\", \"Sequence\", \"peptide\", \"Peptide\", \"seq\", \"SEQ\"],\n",
        "        \"sequence\",\n",
        "    )\n",
        "    y_col = _pick_column(\n",
        "        df,\n",
        "        label_col,\n",
        "        [\"label\", \"Label\", \"aggregation\", \"Aggregation\", \"y\", \"Y\"],\n",
        "        \"label\",\n",
        "    )\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.isna(row[seq_col]) or pd.isna(row[y_col]):\n",
        "            continue\n",
        "        seq = str(row[seq_col]).strip()\n",
        "        if not seq:\n",
        "            continue\n",
        "        y = int(row[y_col])\n",
        "        rows.append((seq, y))\n",
        "\n",
        "    return rows, seq_col, y_col\n",
        "\n",
        "\n",
        "def build_atom_level_dataset(\n",
        "    peptide_excel_path,\n",
        "    res_dict,\n",
        "    sequence_col=None,\n",
        "    label_col=None,\n",
        "    strict=True,\n",
        "):\n",
        "    rows, used_seq_col, used_label_col = load_peptide_rows(\n",
        "        peptide_excel_path,\n",
        "        sequence_col=sequence_col,\n",
        "        label_col=label_col,\n",
        "    )\n",
        "\n",
        "    data_list = []\n",
        "    dropped = 0\n",
        "\n",
        "    for seq, y in rows:\n",
        "        try:\n",
        "            data = build_peptide_graph(seq, res_dict)\n",
        "            data.y = torch.tensor([y], dtype=torch.float)\n",
        "            data.sequence = seq\n",
        "            data_list.append(data)\n",
        "        except Exception as e:\n",
        "            if strict:\n",
        "                raise RuntimeError(f\"Failed on sequence '{seq}': {e}\")\n",
        "            dropped += 1\n",
        "\n",
        "    if len(data_list) == 0:\n",
        "        raise ValueError(\"No valid graph samples were generated.\")\n",
        "\n",
        "    print(f\"Loaded {len(rows)} rows from peptide excel\")\n",
        "    print(f\"Using sequence column: {used_seq_col}\")\n",
        "    print(f\"Using label column: {used_label_col}\")\n",
        "    print(f\"Built {len(data_list)} graphs (dropped={dropped})\")\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def stratified_split_data_list(data_list, random_state=42):\n",
        "    labels = [int(d.y.item()) for d in data_list]\n",
        "    indices = list(range(len(data_list)))\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.3,\n",
        "        stratify=labels,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=[labels[i] for i in temp_idx],\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    train_data = [data_list[i] for i in train_idx]\n",
        "    val_data = [data_list[i] for i in val_idx]\n",
        "    test_data = [data_list[i] for i in test_idx]\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def save_splits(train_data, val_data, test_data, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    torch.save({\"data_list\": train_data}, os.path.join(save_dir, \"atom_level_train.pt\"))\n",
        "    torch.save({\"data_list\": val_data}, os.path.join(save_dir, \"atom_level_val.pt\"))\n",
        "    torch.save({\"data_list\": test_data}, os.path.join(save_dir, \"atom_level_test.pt\"))\n",
        "\n",
        "    print(f\"Saved splits to: {save_dir}\")\n",
        "    print(f\"train/val/test = {len(train_data)}/{len(val_data)}/{len(test_data)}\")\n",
        "    print(f\"Sample x shape: {train_data[0].x.shape}\")\n",
        "    print(f\"Sample edge_attr shape: {train_data[0].edge_attr.shape}\")\n",
        "\n",
        "\n",
        "def main_build_atom_level_dataset():\n",
        "    # 1) Build residue dictionary: AA code -> capped residue SMILES\n",
        "    res_dict = load_residue_dictionary_from_excel(\n",
        "        RESIDUAL_DICT_XLSX,\n",
        "        code_col=AA_CODE_COL,\n",
        "        smiles_col=AA_SMILES_COL,\n",
        "    )\n",
        "\n",
        "    # 2) Build atom-level graph list from peptide sequences\n",
        "    data_list = build_atom_level_dataset(\n",
        "        PEPTIDE_EXCEL_PATH,\n",
        "        res_dict,\n",
        "        sequence_col=SEQUENCE_COL,\n",
        "        label_col=LABEL_COL,\n",
        "        strict=True,\n",
        "    )\n",
        "\n",
        "    # 3) 70/15/15 stratified split with fixed random seed\n",
        "    train_data, val_data, test_data = stratified_split_data_list(data_list, random_state=42)\n",
        "\n",
        "    # 4) Save .pt files\n",
        "    save_splits(train_data, val_data, test_data, SAVE_DIR)\n",
        "\n",
        "\n",
        "# Run when you are ready.\n",
        "main_build_atom_level_dataset()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGOoAPiNIWd"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_smiles_in_notebook"
      },
      "source": [
        "model_smiles.py code (copied for notebook use, source file unchanged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_smiles_code_cell"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINEConv, global_add_pool, global_mean_pool\n",
        "\n",
        "\n",
        "def _make_gine_mlp(hidden_dim: int) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "    )\n",
        "\n",
        "\n",
        "class AtomEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode per-atom categorical features (Table S2) using one embedding per field,\n",
        "    then sum them into a single hidden representation.\n",
        "\n",
        "    Expected atom feature order in data.x:\n",
        "      0 Atom Type\n",
        "      1 Chirality\n",
        "      2 Total Degree\n",
        "      3 Formal Charge\n",
        "      4 Total Number of Hs\n",
        "      5 Number of Radical Electrons\n",
        "      6 Hybridization\n",
        "      7 Aromatic (bool)\n",
        "      8 Part of Ring (bool)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, feature_dims=None):\n",
        "        super().__init__()\n",
        "        if feature_dims is None:\n",
        "            # Must match preprocessing vocab sizes.\n",
        "            feature_dims = [14, 5, 8, 8, 6, 4, 6, 2, 2]\n",
        "\n",
        "        self.feature_dims = feature_dims\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(dim, hidden_dim) for dim in feature_dims\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for emb in self.embeddings:\n",
        "            nn.init.xavier_uniform_(emb.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() != 2:\n",
        "            raise ValueError(f\"Expected x with shape [num_nodes, num_atom_features], got {x.shape}\")\n",
        "        if x.size(1) != len(self.embeddings):\n",
        "            raise ValueError(\n",
        "                f\"Expected {len(self.embeddings)} atom feature columns, got {x.size(1)}\"\n",
        "            )\n",
        "\n",
        "        x = x.long()\n",
        "        out = 0\n",
        "        for i, emb in enumerate(self.embeddings):\n",
        "            out = out + emb(x[:, i])\n",
        "        return out\n",
        "\n",
        "\n",
        "class EdgeEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode per-bond categorical features (Table S3) using one embedding per field,\n",
        "    then sum them into a hidden representation used by GINEConv.\n",
        "\n",
        "    Expected edge feature order in data.edge_attr:\n",
        "      0 Bond Type\n",
        "      1 Bond Stereo\n",
        "      2 Conjugation (bool)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, feature_dims=None):\n",
        "        super().__init__()\n",
        "        if feature_dims is None:\n",
        "            # Must match preprocessing vocab sizes.\n",
        "            feature_dims = [5, 7, 2]\n",
        "\n",
        "        self.feature_dims = feature_dims\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(dim, hidden_dim) for dim in feature_dims\n",
        "        ])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for emb in self.embeddings:\n",
        "            nn.init.xavier_uniform_(emb.weight)\n",
        "\n",
        "    def forward(self, edge_attr: torch.Tensor) -> torch.Tensor:\n",
        "        if edge_attr.dim() != 2:\n",
        "            raise ValueError(\n",
        "                f\"Expected edge_attr with shape [num_edges, num_edge_features], got {edge_attr.shape}\"\n",
        "            )\n",
        "        if edge_attr.size(1) != len(self.embeddings):\n",
        "            raise ValueError(\n",
        "                f\"Expected {len(self.embeddings)} edge feature columns, got {edge_attr.size(1)}\"\n",
        "            )\n",
        "\n",
        "        edge_attr = edge_attr.long()\n",
        "        out = 0\n",
        "        for i, emb in enumerate(self.embeddings):\n",
        "            out = out + emb(edge_attr[:, i])\n",
        "        return out\n",
        "\n",
        "\n",
        "class GINEVirtualNodeClassifierAtom(nn.Module):\n",
        "    \"\"\"\n",
        "    Atom-level GINEConv + Virtual Node classifier for graph-level binary prediction.\n",
        "\n",
        "    Input:\n",
        "      data.x:        [num_nodes, 9]   categorical atom features\n",
        "      data.edge_attr:[num_edges, 3]   categorical bond features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 128,\n",
        "        num_layers: int = 4,\n",
        "        dropout: float = 0.2,\n",
        "        pooling: str = \"mean\",\n",
        "        atom_feature_dims=None,\n",
        "        edge_feature_dims=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if pooling not in [\"mean\", \"add\"]:\n",
        "            raise ValueError(f\"pooling must be 'mean' or 'add', got {pooling}\")\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.atom_encoder = AtomEncoder(hidden_dim=hidden_dim, feature_dims=atom_feature_dims)\n",
        "        self.edge_encoder = EdgeEncoder(hidden_dim=hidden_dim, feature_dims=edge_feature_dims)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [GINEConv(_make_gine_mlp(hidden_dim)) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.vn_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        x = self.atom_encoder(x)\n",
        "        e = self.edge_encoder(edge_attr)\n",
        "\n",
        "        num_graphs = int(batch.max().item()) + 1 if batch.numel() > 0 else 0\n",
        "        virtualnode_emb = x.new_zeros((num_graphs, self.hidden_dim))\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = x + virtualnode_emb[batch]\n",
        "            x = self.convs[i](x, edge_index, e)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "            if i != self.num_layers - 1:\n",
        "                vn_update = global_add_pool(x, batch)\n",
        "                virtualnode_emb = virtualnode_emb + self.vn_mlp(vn_update)\n",
        "\n",
        "        graph_emb = global_add_pool(x, batch) if self.pooling == \"add\" else global_mean_pool(x, batch)\n",
        "        logits = self.classifier(graph_emb).view(-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Backward-compatible class name\n",
        "class GINEVirtualNodeClassifier(GINEVirtualNodeClassifierAtom):\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_atom_gnn_in_notebook"
      },
      "source": [
        "train_atom_gnn.py code (saved file + notebook copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_atom_gnn_code_cell",
        "outputId": "17a9c6fa-330f-4eed-83e1-b2a2546e642f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3294642911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3294642911.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mModelCls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mutils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_import_train_utils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3294642911.py\u001b[0m in \u001b[0;36m_load_model_class\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;34m...\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata_py_file\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel_smiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mthis_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mdata_py_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_py_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"atom-level-smiles\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_smiles.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "import importlib.util\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "\n",
        "# Atom-level dataset built from notebook pipeline\n",
        "TRAIN_PT = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles/atom_level_train.pt\"\n",
        "VAL_PT = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles/atom_level_val.pt\"\n",
        "TEST_PT = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles/atom_level_test.pt\"\n",
        "\n",
        "LOG_DIR = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles/logs_tensorboard\"\n",
        "BEST_DIR = \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/atom-level-smiles/BEST_MODEL\"\n",
        "\n",
        "\n",
        "def _resolve_data_py_dir():\n",
        "    \"\"\"\n",
        "    Resolve .../GNN/data_py_file in both script and notebook execution modes.\n",
        "    \"\"\"\n",
        "    if \"__file__\" in globals():\n",
        "        this_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "        return os.path.dirname(this_dir)\n",
        "\n",
        "    candidates = [\n",
        "        \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/data_py_file\",\n",
        "        os.path.join(\n",
        "            os.getcwd(),\n",
        "            \"My Drive/master_thesis/sampled_data_5000/GNN/data_py_file\",\n",
        "        ),\n",
        "        os.path.join(os.getcwd(), \"data_py_file\"),\n",
        "    ]\n",
        "    for path in candidates:\n",
        "        if os.path.isdir(path):\n",
        "            return path\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Cannot locate '.../GNN/data_py_file'. \"\n",
        "        \"Set working directory to project root or run this file as a script.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def _load_model_class():\n",
        "    \"\"\"\n",
        "    Load model class from sibling folder:\n",
        "      .../data_py_file/atom-level-smiles/model_smiles.py\n",
        "    \"\"\"\n",
        "    data_py_dir = _resolve_data_py_dir()\n",
        "    model_path = os.path.join(data_py_dir, \"atom-level-smiles\", \"model_smiles.py\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"model_smiles.py not found at: {model_path}\")\n",
        "\n",
        "    spec = importlib.util.spec_from_file_location(\"model_smiles\", model_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "\n",
        "    if hasattr(module, \"GINEVirtualNodeClassifierAtom\"):\n",
        "        return module.GINEVirtualNodeClassifierAtom\n",
        "    if hasattr(module, \"GINEVirtualNodeClassifier\"):\n",
        "        return module.GINEVirtualNodeClassifier\n",
        "\n",
        "    raise AttributeError(\"No compatible classifier class found in model_smiles.py\")\n",
        "\n",
        "\n",
        "def _import_train_utils():\n",
        "    data_py_dir = _resolve_data_py_dir()\n",
        "    if data_py_dir not in sys.path:\n",
        "        sys.path.insert(0, data_py_dir)\n",
        "\n",
        "    from train_utils import (\n",
        "        compute_pos_weight,\n",
        "        evaluate,\n",
        "        final_test_report,\n",
        "        find_best_threshold_f1,\n",
        "        fit_with_validation,\n",
        "        set_seed,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"compute_pos_weight\": compute_pos_weight,\n",
        "        \"evaluate\": evaluate,\n",
        "        \"final_test_report\": final_test_report,\n",
        "        \"find_best_threshold_f1\": find_best_threshold_f1,\n",
        "        \"fit_with_validation\": fit_with_validation,\n",
        "        \"set_seed\": set_seed,\n",
        "    }\n",
        "\n",
        "\n",
        "def build_loaders_with_seed_pyg(train_dataset, val_dataset, test_dataset, batch_size, seed):\n",
        "    gen = torch.Generator()\n",
        "    gen.manual_seed(seed)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=gen)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def mean_std(vals):\n",
        "    vals = np.array(vals, dtype=float)\n",
        "    return vals.mean(), vals.std(ddof=1) if len(vals) > 1 else 0.0\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.makedirs(BEST_DIR, exist_ok=True)\n",
        "\n",
        "    train_list = torch.load(TRAIN_PT, map_location=\"cpu\", weights_only=False)[\"data_list\"]\n",
        "    val_list = torch.load(VAL_PT, map_location=\"cpu\", weights_only=False)[\"data_list\"]\n",
        "    test_list = torch.load(TEST_PT, map_location=\"cpu\", weights_only=False)[\"data_list\"]\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"GPU available\")\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    ModelCls = _load_model_class()\n",
        "    utils = _import_train_utils()\n",
        "\n",
        "    seeds = [0, 1, 2]\n",
        "    results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n=== Run seed={seed} ===\")\n",
        "        utils[\"set_seed\"](seed)\n",
        "\n",
        "        train_loader, val_loader, test_loader = build_loaders_with_seed_pyg(\n",
        "            train_list, val_list, test_list, batch_size=32, seed=seed\n",
        "        )\n",
        "\n",
        "        model = ModelCls(\n",
        "            hidden_dim=64,\n",
        "            num_layers=3,\n",
        "            dropout=0.2,\n",
        "            pooling=\"mean\",\n",
        "        )\n",
        "\n",
        "        pos_weight = utils[\"compute_pos_weight\"](train_loader, device)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        exp_prefix = \"atom_01\"\n",
        "        run_prefix = f\"{exp_prefix}-seed{seed:02d}\"\n",
        "        best_model_path = os.path.join(BEST_DIR, f\"{run_prefix}_best_model.pt\")\n",
        "\n",
        "        utils[\"fit_with_validation\"](\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            device=device,\n",
        "            optimizer=optimizer,\n",
        "            loss_fn=loss_fn,\n",
        "            epochs=200,\n",
        "            patience=10,\n",
        "            model_path=best_model_path,\n",
        "            log_dir=LOG_DIR,\n",
        "            run_prefix=run_prefix,\n",
        "        )\n",
        "\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "        model = model.to(device)\n",
        "\n",
        "        val_metrics = utils[\"evaluate\"](model, val_loader, device, loss_fn, threshold=0.5)\n",
        "        best_t, best_f1, _, _ = utils[\"find_best_threshold_f1\"](\n",
        "            val_metrics[\"probs\"], val_metrics[\"labels\"]\n",
        "        )\n",
        "        test_metrics = utils[\"evaluate\"](model, test_loader, device, loss_fn, threshold=best_t)\n",
        "\n",
        "        utils[\"final_test_report\"](\n",
        "            model=model,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            device=device,\n",
        "            loss_fn=loss_fn,\n",
        "            model_path=best_model_path,\n",
        "        )\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"seed\": seed,\n",
        "                \"val_auc\": val_metrics[\"roc_auc\"],\n",
        "                \"val_pr_auc\": val_metrics[\"pr_auc\"],\n",
        "                \"val_f1\": best_f1,\n",
        "                \"val_acc\": val_metrics[\"accuracy\"],\n",
        "                \"test_auc\": test_metrics[\"roc_auc\"],\n",
        "                \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
        "                \"test_f1\": test_metrics[\"f1\"],\n",
        "                \"test_acc\": test_metrics[\"accuracy\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\\n=== Summary over seeds ===\")\n",
        "    for split in [\"val\", \"test\"]:\n",
        "        for key in [\"auc\", \"pr_auc\", \"f1\", \"acc\"]:\n",
        "            m, s = mean_std([r[f\"{split}_{key}\"] for r in results])\n",
        "            print(f\"{split}/{key}: {m:.4f} ± {s:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1jGPU18U5Dp",
        "outputId": "b68e82e0-bfdd-4ae3-f272-9f1feaab394c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "Using CPU\n",
            "2026-02-19 14:53:55.399807: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-02-19 14:53:55.925781: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-02-19 14:53:56.227986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771512836.285137    5984 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771512836.297215    5984 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771512836.332722    5984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771512836.332790    5984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771512836.332803    5984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771512836.332844    5984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-19 14:53:56.348415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "=== Run seed=0 ===\n",
            "Epoch001 | lr=0.000500 | train_loss=1.0908 | train_auc=0.5981 val_loss=1.0811 | val_auc=0.6059 | val_f1=0.3685\n",
            "Epoch002 | lr=0.000500 | train_loss=1.0852 | train_auc=0.6081 val_loss=1.0747 | val_auc=0.5979 | val_f1=0.3620\n",
            "Epoch003 | lr=0.000500 | train_loss=1.0722 | train_auc=0.6209 val_loss=1.0877 | val_auc=0.5933 | val_f1=0.2927\n",
            "Epoch004 | lr=0.000500 | train_loss=1.0681 | train_auc=0.6273 val_loss=1.0771 | val_auc=0.6048 | val_f1=0.3661\n",
            "Epoch005 | lr=0.000500 | train_loss=1.0727 | train_auc=0.6452 val_loss=1.0809 | val_auc=0.6235 | val_f1=0.2540\n",
            "Epoch006 | lr=0.000500 | train_loss=1.0620 | train_auc=0.6369 val_loss=1.0658 | val_auc=0.6351 | val_f1=0.3703\n",
            "Epoch007 | lr=0.000500 | train_loss=1.0635 | train_auc=0.6487 val_loss=1.0686 | val_auc=0.6128 | val_f1=0.3279\n",
            "Epoch008 | lr=0.000500 | train_loss=1.0543 | train_auc=0.6451 val_loss=1.1541 | val_auc=0.5855 | val_f1=0.2069\n",
            "Epoch009 | lr=0.000500 | train_loss=1.0578 | train_auc=0.6615 val_loss=1.0757 | val_auc=0.6003 | val_f1=0.3325\n",
            "Epoch010 | lr=0.000500 | train_loss=1.0515 | train_auc=0.6523 val_loss=1.1236 | val_auc=0.5950 | val_f1=0.2054\n",
            "Epoch011 | lr=0.000500 | train_loss=1.0480 | train_auc=0.6612 val_loss=1.1106 | val_auc=0.5988 | val_f1=0.3355\n",
            "Epoch012 | lr=0.000250 | train_loss=1.0385 | train_auc=0.6425 val_loss=1.1047 | val_auc=0.5908 | val_f1=0.3601\n",
            "Epoch013 | lr=0.000250 | train_loss=1.0308 | train_auc=0.6865 val_loss=1.0719 | val_auc=0.6213 | val_f1=0.3633\n",
            "Epoch014 | lr=0.000250 | train_loss=1.0380 | train_auc=0.6822 val_loss=1.0650 | val_auc=0.6358 | val_f1=0.3726\n",
            "Epoch015 | lr=0.000250 | train_loss=1.0299 | train_auc=0.6736 val_loss=1.1150 | val_auc=0.6025 | val_f1=0.2814\n",
            "Epoch016 | lr=0.000250 | train_loss=1.0260 | train_auc=0.6830 val_loss=1.0727 | val_auc=0.6254 | val_f1=0.3641\n",
            "Epoch017 | lr=0.000250 | train_loss=1.0290 | train_auc=0.6840 val_loss=1.0915 | val_auc=0.6302 | val_f1=0.3758\n",
            "Epoch018 | lr=0.000250 | train_loss=1.0270 | train_auc=0.6934 val_loss=1.0598 | val_auc=0.6350 | val_f1=0.3715\n",
            "Epoch019 | lr=0.000250 | train_loss=1.0296 | train_auc=0.6895 val_loss=1.0709 | val_auc=0.6253 | val_f1=0.3136\n",
            "Epoch020 | lr=0.000125 | train_loss=1.0214 | train_auc=0.6853 val_loss=1.0799 | val_auc=0.6250 | val_f1=0.3648\n",
            "Epoch021 | lr=0.000125 | train_loss=1.0203 | train_auc=0.6980 val_loss=1.0620 | val_auc=0.6354 | val_f1=0.3684\n",
            "Epoch022 | lr=0.000125 | train_loss=1.0138 | train_auc=0.7003 val_loss=1.0580 | val_auc=0.6294 | val_f1=0.3521\n",
            "Epoch023 | lr=0.000125 | train_loss=1.0175 | train_auc=0.7007 val_loss=1.0704 | val_auc=0.6246 | val_f1=0.3693\n",
            "Epoch024 | lr=0.000125 | train_loss=1.0164 | train_auc=0.6979 val_loss=1.0724 | val_auc=0.6203 | val_f1=0.3429\n",
            "Early stopping at epoch 24\n",
            "Best threshold on val set: threshold=0.58 | F1=0.3835 | Precision=0.3074 | Recall=0.5097\n",
            "\n",
            "Test metrics:\n",
            "roc_auc: 0.6605\n",
            "pr_auc: 0.3849\n",
            "accuracy: 0.6800\n",
            "precision: 0.3396\n",
            "recall: 0.5806\n",
            "f1: 0.4286\n",
            "confusion matrix:\n",
            "[[420 175]\n",
            " [ 65  90]]\n",
            "\n",
            "=== Run seed=1 ===\n",
            "Epoch001 | lr=0.000500 | train_loss=1.0890 | train_auc=0.5882 val_loss=1.0774 | val_auc=0.6098 | val_f1=0.3731\n",
            "Epoch002 | lr=0.000500 | train_loss=1.0807 | train_auc=0.6045 val_loss=1.0657 | val_auc=0.6239 | val_f1=0.3651\n",
            "Epoch003 | lr=0.000500 | train_loss=1.0696 | train_auc=0.5975 val_loss=1.0932 | val_auc=0.5724 | val_f1=0.0792\n",
            "Epoch004 | lr=0.000500 | train_loss=1.0738 | train_auc=0.6336 val_loss=1.0904 | val_auc=0.5941 | val_f1=0.3304\n",
            "Epoch005 | lr=0.000500 | train_loss=1.0648 | train_auc=0.6146 val_loss=1.1329 | val_auc=0.5982 | val_f1=0.3697\n",
            "Epoch006 | lr=0.000500 | train_loss=1.0571 | train_auc=0.6385 val_loss=1.0735 | val_auc=0.6028 | val_f1=0.3500\n",
            "Epoch007 | lr=0.000500 | train_loss=1.0513 | train_auc=0.6461 val_loss=1.1549 | val_auc=0.5959 | val_f1=0.1712\n",
            "Epoch008 | lr=0.000250 | train_loss=1.0507 | train_auc=0.6600 val_loss=1.0751 | val_auc=0.5965 | val_f1=0.3289\n",
            "Epoch009 | lr=0.000250 | train_loss=1.0413 | train_auc=0.6663 val_loss=1.1332 | val_auc=0.6368 | val_f1=0.3586\n",
            "Epoch010 | lr=0.000250 | train_loss=1.0344 | train_auc=0.6735 val_loss=1.0708 | val_auc=0.6233 | val_f1=0.3707\n",
            "Epoch011 | lr=0.000250 | train_loss=1.0293 | train_auc=0.6759 val_loss=1.0934 | val_auc=0.6207 | val_f1=0.3392\n",
            "Epoch012 | lr=0.000250 | train_loss=1.0280 | train_auc=0.6547 val_loss=1.1331 | val_auc=0.6099 | val_f1=0.3129\n",
            "Epoch013 | lr=0.000250 | train_loss=1.0336 | train_auc=0.6830 val_loss=1.0614 | val_auc=0.6444 | val_f1=0.4065\n",
            "Epoch014 | lr=0.000250 | train_loss=1.0241 | train_auc=0.6758 val_loss=1.1018 | val_auc=0.6306 | val_f1=0.3093\n",
            "Epoch015 | lr=0.000250 | train_loss=1.0344 | train_auc=0.6814 val_loss=1.0699 | val_auc=0.6246 | val_f1=0.3733\n",
            "Epoch016 | lr=0.000250 | train_loss=1.0348 | train_auc=0.6788 val_loss=1.0888 | val_auc=0.6410 | val_f1=0.3761\n",
            "Epoch017 | lr=0.000250 | train_loss=1.0262 | train_auc=0.6835 val_loss=1.0850 | val_auc=0.6328 | val_f1=0.3791\n",
            "Epoch018 | lr=0.000250 | train_loss=1.0263 | train_auc=0.6700 val_loss=1.1106 | val_auc=0.6370 | val_f1=0.3846\n",
            "Epoch019 | lr=0.000125 | train_loss=1.0289 | train_auc=0.6927 val_loss=1.0667 | val_auc=0.6380 | val_f1=0.3286\n",
            "Epoch020 | lr=0.000125 | train_loss=1.0157 | train_auc=0.6955 val_loss=1.0574 | val_auc=0.6412 | val_f1=0.3804\n",
            "Epoch021 | lr=0.000125 | train_loss=1.0222 | train_auc=0.6944 val_loss=1.0569 | val_auc=0.6459 | val_f1=0.3901\n",
            "Epoch022 | lr=0.000125 | train_loss=1.0159 | train_auc=0.7041 val_loss=1.0525 | val_auc=0.6413 | val_f1=0.3902\n",
            "Epoch023 | lr=0.000125 | train_loss=1.0139 | train_auc=0.6938 val_loss=1.0717 | val_auc=0.6272 | val_f1=0.3107\n",
            "Epoch024 | lr=0.000125 | train_loss=1.0150 | train_auc=0.6835 val_loss=1.0951 | val_auc=0.6243 | val_f1=0.2748\n",
            "Epoch025 | lr=0.000125 | train_loss=1.0150 | train_auc=0.7043 val_loss=1.0588 | val_auc=0.6364 | val_f1=0.3546\n",
            "Epoch026 | lr=0.000125 | train_loss=1.0184 | train_auc=0.7040 val_loss=1.0588 | val_auc=0.6400 | val_f1=0.3624\n",
            "Epoch027 | lr=0.000063 | train_loss=1.0152 | train_auc=0.7055 val_loss=1.0612 | val_auc=0.6412 | val_f1=0.3768\n",
            "Epoch028 | lr=0.000063 | train_loss=1.0119 | train_auc=0.7126 val_loss=1.0644 | val_auc=0.6392 | val_f1=0.3558\n",
            "Epoch029 | lr=0.000063 | train_loss=1.0129 | train_auc=0.7137 val_loss=1.0577 | val_auc=0.6442 | val_f1=0.3860\n",
            "Epoch030 | lr=0.000063 | train_loss=0.9939 | train_auc=0.7148 val_loss=1.0606 | val_auc=0.6435 | val_f1=0.3943\n",
            "Epoch031 | lr=0.000063 | train_loss=0.9980 | train_auc=0.7085 val_loss=1.0688 | val_auc=0.6404 | val_f1=0.3715\n",
            "Early stopping at epoch 31\n",
            "Best threshold on val set: threshold=0.49 | F1=0.3945 | Precision=0.2687 | Recall=0.7419\n",
            "\n",
            "Test metrics:\n",
            "roc_auc: 0.6619\n",
            "pr_auc: 0.4052\n",
            "accuracy: 0.5373\n",
            "precision: 0.2681\n",
            "recall: 0.7161\n",
            "f1: 0.3902\n",
            "confusion matrix:\n",
            "[[292 303]\n",
            " [ 44 111]]\n",
            "\n",
            "=== Run seed=2 ===\n",
            "Epoch001 | lr=0.000500 | train_loss=1.0849 | train_auc=0.6060 val_loss=1.0956 | val_auc=0.6071 | val_f1=0.0559\n",
            "Epoch002 | lr=0.000500 | train_loss=1.0730 | train_auc=0.6281 val_loss=1.0758 | val_auc=0.6013 | val_f1=0.3588\n",
            "Epoch003 | lr=0.000500 | train_loss=1.0656 | train_auc=0.6361 val_loss=1.1890 | val_auc=0.6130 | val_f1=0.3694\n",
            "Epoch004 | lr=0.000500 | train_loss=1.0557 | train_auc=0.6325 val_loss=1.0909 | val_auc=0.5979 | val_f1=0.3525\n",
            "Epoch005 | lr=0.000500 | train_loss=1.0511 | train_auc=0.6440 val_loss=1.0676 | val_auc=0.6216 | val_f1=0.3473\n",
            "Epoch006 | lr=0.000500 | train_loss=1.0559 | train_auc=0.6553 val_loss=1.0793 | val_auc=0.6143 | val_f1=0.3649\n",
            "Epoch007 | lr=0.000500 | train_loss=1.0514 | train_auc=0.6178 val_loss=1.1979 | val_auc=0.5819 | val_f1=0.1584\n",
            "Epoch008 | lr=0.000500 | train_loss=1.0482 | train_auc=0.6334 val_loss=1.1039 | val_auc=0.5884 | val_f1=0.1966\n",
            "Epoch009 | lr=0.000500 | train_loss=1.0433 | train_auc=0.6220 val_loss=1.2101 | val_auc=0.5735 | val_f1=0.1058\n",
            "Epoch010 | lr=0.000500 | train_loss=1.0332 | train_auc=0.6587 val_loss=1.0926 | val_auc=0.6302 | val_f1=0.3292\n",
            "Epoch011 | lr=0.000500 | train_loss=1.0424 | train_auc=0.6598 val_loss=1.0782 | val_auc=0.5974 | val_f1=0.3184\n",
            "Epoch012 | lr=0.000500 | train_loss=1.0337 | train_auc=0.6648 val_loss=1.0616 | val_auc=0.6134 | val_f1=0.3441\n",
            "Epoch013 | lr=0.000500 | train_loss=1.0328 | train_auc=0.6528 val_loss=1.0928 | val_auc=0.6111 | val_f1=0.3396\n",
            "Epoch014 | lr=0.000500 | train_loss=1.0311 | train_auc=0.6870 val_loss=1.0651 | val_auc=0.6285 | val_f1=0.3704\n",
            "Epoch015 | lr=0.000500 | train_loss=1.0238 | train_auc=0.6463 val_loss=1.1679 | val_auc=0.5889 | val_f1=0.2500\n",
            "Epoch016 | lr=0.000250 | train_loss=1.0247 | train_auc=0.6489 val_loss=1.1397 | val_auc=0.6137 | val_f1=0.3046\n",
            "Epoch017 | lr=0.000250 | train_loss=1.0223 | train_auc=0.6908 val_loss=1.0577 | val_auc=0.6427 | val_f1=0.3889\n",
            "Epoch018 | lr=0.000250 | train_loss=1.0214 | train_auc=0.6777 val_loss=1.1173 | val_auc=0.6226 | val_f1=0.3669\n",
            "Epoch019 | lr=0.000250 | train_loss=1.0240 | train_auc=0.6771 val_loss=1.0717 | val_auc=0.6256 | val_f1=0.3535\n",
            "Epoch020 | lr=0.000250 | train_loss=1.0152 | train_auc=0.6701 val_loss=1.1333 | val_auc=0.6256 | val_f1=0.3642\n",
            "Epoch021 | lr=0.000250 | train_loss=1.0184 | train_auc=0.7070 val_loss=1.0651 | val_auc=0.6306 | val_f1=0.3587\n",
            "Epoch022 | lr=0.000250 | train_loss=1.0142 | train_auc=0.6962 val_loss=1.0582 | val_auc=0.6531 | val_f1=0.3878\n",
            "Epoch023 | lr=0.000250 | train_loss=1.0144 | train_auc=0.6978 val_loss=1.0655 | val_auc=0.6242 | val_f1=0.3881\n",
            "Epoch024 | lr=0.000250 | train_loss=1.0190 | train_auc=0.7042 val_loss=1.0663 | val_auc=0.6358 | val_f1=0.3718\n",
            "Epoch025 | lr=0.000250 | train_loss=1.0160 | train_auc=0.6871 val_loss=1.0624 | val_auc=0.6225 | val_f1=0.3453\n",
            "Epoch026 | lr=0.000250 | train_loss=1.0066 | train_auc=0.7090 val_loss=1.0516 | val_auc=0.6438 | val_f1=0.3888\n",
            "Epoch027 | lr=0.000250 | train_loss=1.0128 | train_auc=0.7159 val_loss=1.0565 | val_auc=0.6377 | val_f1=0.3789\n",
            "Epoch028 | lr=0.000125 | train_loss=1.0141 | train_auc=0.7122 val_loss=1.0632 | val_auc=0.6359 | val_f1=0.3628\n",
            "Epoch029 | lr=0.000125 | train_loss=1.0050 | train_auc=0.7117 val_loss=1.0581 | val_auc=0.6407 | val_f1=0.3839\n",
            "Epoch030 | lr=0.000125 | train_loss=1.0003 | train_auc=0.7048 val_loss=1.0783 | val_auc=0.6382 | val_f1=0.3821\n",
            "Epoch031 | lr=0.000125 | train_loss=1.0009 | train_auc=0.7205 val_loss=1.0643 | val_auc=0.6335 | val_f1=0.3733\n",
            "Epoch032 | lr=0.000125 | train_loss=0.9917 | train_auc=0.7141 val_loss=1.0716 | val_auc=0.6363 | val_f1=0.3875\n",
            "Early stopping at epoch 32\n",
            "Best threshold on val set: threshold=0.54 | F1=0.3976 | Precision=0.2861 | Recall=0.6516\n",
            "\n",
            "Test metrics:\n",
            "roc_auc: 0.6651\n",
            "pr_auc: 0.4181\n",
            "accuracy: 0.6040\n",
            "precision: 0.2948\n",
            "recall: 0.6581\n",
            "f1: 0.4072\n",
            "confusion matrix:\n",
            "[[351 244]\n",
            " [ 53 102]]\n",
            "\n",
            "=== Summary over seeds ===\n",
            "val/auc: 0.6449 ± 0.0087\n",
            "val/pr_auc: 0.3153 ± 0.0104\n",
            "val/f1: 0.3919 ± 0.0074\n",
            "val/acc: 0.5489 ± 0.0098\n",
            "test/auc: 0.6625 ± 0.0024\n",
            "test/pr_auc: 0.4027 ± 0.0167\n",
            "test/f1: 0.4086 ± 0.0192\n",
            "test/acc: 0.6071 ± 0.0714\n"
          ]
        }
      ],
      "source": [
        "# 方式1：推荐，直接跑脚本文件\n",
        "!python \"/content/drive/MyDrive/master_thesis/sampled_data_5000/GNN/data_py_file/atom_level_smiles/train_atom_gnn.py\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}